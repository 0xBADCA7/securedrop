#!/usr/bin/python2.7
"""
This script is copied to the App server and run by the Ansible playbook. When
run (as root), it collects the minimum information necessary to backup the
source identities to enable them to login.
"""

from datetime import datetime
import json
import subprocess
import os
import sys

import config
from db import Source


def validate_source(source):
    # Is the filesystem_id a valid hash?
    valid_hash_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890='
    for char in source.filesystem_id:
        assert char in valid_hash_characters

    valid_codeword_characters = "'abcdefghijklmnopqrstuvwxyz0123456789-_"
    # Does the journalist designation consist of expected words?
    for word in source.journalist_designation.split():
        for char in word:
            assert str.lower(str(char)) in valid_codeword_characters

    return source.filesystem_id, source.journalist_designation


def main():
    backup_datetime = datetime.utcnow().strftime("%Y-%m-%d--%H-%M-%S")
    backup_filename = 'sd-source-backup-{}.tar.gz'.format(backup_datetime)

    sources = Source.query.all()
    sources_export = []

    # Now go through each source and assert that the data is what we expect
    for source in sources:
        try:
            filesystem_id, journalist_designation = validate_source(source)
        except AssertionError:
            print ('Unusual data found in a sources table. '
                   'Please contact Freedom of the Press Foundation.')
            sys.exit(1)

        # Export armored source secret key
        gpg_key = subprocess.check_output(
            ['gpg', '--homedir', config.GPG_KEY_DIR, '--armor',
             '--export-secret-key',
             "Autogenerated Key <{}>".format(filesystem_id)])

        source_data = {'filesystem_id': filesystem_id,
                       'journalist_designation': journalist_designation,
                       'gpg_key': gpg_key}
        sources_export.append(source_data)

    # Save a JSON file of the source data we need to import their identities
    backup_filename = 'sources-backup-{}.json'.format(backup_datetime)
    with open(backup_filename, 'w') as outfile:
        json.dump(sources_export, outfile)

    # Make all files in gpg home folder owned by www-data
    gpg_files = [f for f in os.listdir(config.GPG_KEY_DIR)
        if os.path.isdir(os.path.join(config.GPG_KEY_DIR, f))]

    for gpg_file in gpg_files:
        uid = pwd.getpwnam("www-data").pw_uid
        gid = grp.getgrnam("www-data").gr_gid
        os.chown(gpg_file, uid, gid)

    print backup_filename


if __name__ == "__main__":
    main()
